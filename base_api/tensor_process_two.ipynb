{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. inplace和广播机制\n",
    "\n",
    "- in-place 操作\n",
    "    1. 就地操作, 不适用临时变量, 比如 a.add_(b) = a + b => 计算结果直接赋值给 a 等价于 a += b\n",
    "- 广播机制\n",
    "    1. 张量参数可以自动的扩展为相同的大小\n",
    "    2. 广播机制满足的条件\n",
    "        - 每个张量至少有一个维度\n",
    "        - 满足右对齐 :\n",
    "        - 例如 torch.rand(2,1,1) 和 torch.rand(3) , 如果维度的最后一个维度, 比如 torch.rand(2,1,1) 最后一个是1, torch.rand(3)最后一个维度是3\n",
    "        - 满足 最后(右) 维度的任意一个是1或者两个维度相等就满足右对齐\n",
    "    3. torch.rand(2,1,1) 和 torch.rand(3) 会自动对其, 将torch.rand(3)扩展为torch.rand(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "for module in torch,:\n",
    "    print(module.__name__, module.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.9.1+cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 广播机制\n",
    "\n",
    "- 关于计算结果维度问题\n",
    "- (2,1,1,3) + (4,2,3) => shape : (2, 4, 2, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([3])\n",
      "tensor([[2, 2, 2],\n",
      "        [3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 1, 1],\n",
    "    [2, 2, 2]\n",
    "])\n",
    "b = torch.tensor([1, 1, 1])\n",
    "\n",
    "# a : (2, 3)\n",
    "# b : (1, 3)\n",
    "# a + b => c : b 和 a 的第一行和第二行分别相加\n",
    "\n",
    "c = a + b\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. tensor的取整取余的运算\n",
    "\n",
    "- .floor()向下取整数\n",
    "- .ceil()向上取整数\n",
    "- .round()四舍五入>=0.5向上取整, <0.5向下取整.trunc()裁剪，只取整数部分\n",
    "- .frac()只取小数部分\n",
    "- %取余 : a % b, mod(a, b), remainder(a, b)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5728, 0.3311],\n",
      "        [3.1508, 2.8495]])\n",
      "floor =>  tensor([[1., 0.],\n",
      "        [3., 2.]])\n",
      "ceil =>  tensor([[2., 1.],\n",
      "        [4., 3.]])\n",
      "round =>  tensor([[2., 0.],\n",
      "        [3., 3.]])\n",
      "trunc =>  tensor([[1., 0.],\n",
      "        [3., 2.]])\n",
      "frac =>  tensor([[0.5728, 0.3311],\n",
      "        [0.1508, 0.8495]])\n",
      "a % 2 =>  tensor([[1.5728, 0.3311],\n",
      "        [1.1508, 0.8495]])\n",
      "fmod =>  tensor([[1.5728, 0.3311],\n",
      "        [3.1508, 2.8495]])\n",
      "remainder =>  tensor([[1.5728, 0.3311],\n",
      "        [3.1508, 2.8495]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2)\n",
    "a = a * 10\n",
    "print(a)\n",
    "\n",
    "print(\"floor => \", torch.floor(a))\n",
    "print(\"ceil => \", torch.ceil(a))\n",
    "print(\"round => \", torch.round(a))\n",
    "print(\"trunc => \", torch.trunc(a))\n",
    "print(\"frac => \", torch.frac(a))\n",
    "print(\"a % 2 => \", a % 2)\n",
    "\n",
    "b = torch.tensor([[2, 3], [4, 5]],\n",
    "                 dtype=torch.float)\n",
    "print(\"fmod => \", torch.fmod(a, b))\n",
    "print(\"remainder => \", torch.remainder(a, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. tensor的比较运算\n",
    "\n",
    "- 注意 : 比较的时候, 要求两个数据的shape相同\n",
    "- torch.eq(input, other, out=None) 按成员进行等式操作，相同返回True\n",
    "- torch.equal(tensor1, tensor2) 如果tensor1和tensor2有相同的size和elements,则为true\n",
    "- torch.ge(input, other, out=None) input >= other\n",
    "- torch.gt(input, other, out=None) input > other\n",
    "- torch.le(input, other, out=None) input =< other\n",
    "- torch.It(input, other, out=None) input < other\n",
    "- torch.ne(input, other, out=None) input != other 不等于"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [3, 4, 5]])\n",
      "tensor([[2, 3, 4],\n",
      "        [5, 6, 7]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "False\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [3, 4, 5]\n",
    "])\n",
    "\n",
    "b = torch.tensor([\n",
    "    [2, 3, 4],\n",
    "    [5, 6, 7]\n",
    "])\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# 判断是否相等\n",
    "print(torch.eq(a, b))\n",
    "print(torch.equal(a, b))\n",
    "# 大于等于\n",
    "print(torch.ge(a, b))\n",
    "# 大于\n",
    "print(torch.gt(a, b))\n",
    "# 小于等于\n",
    "print(torch.le(a, b))\n",
    "# 小于\n",
    "print(torch.lt(a, b))\n",
    "# 不等于\n",
    "print(torch.ne(a, b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. tensor的排序与前k值运算\n",
    "\n",
    "- torch.sort(input, dim=None, descending=False, out=None) 对目标input进行排序\n",
    "- torch.topk(input, k, dim=None, largest=True, sorted=True,out=None) 沿着指定维度返回最大k个数值及其索引值\n",
    "- torch.kthvalue(input, k, dim=None, out=None) 沿着指定维度返回第k个最小值及其索引值\n",
    "- torch.isfinite(tensor) tensor是否是有界的\n",
    "- torch.isinf(tensor) tensor是否是无界的(无穷大)\n",
    "- torch.isnan(tensor) 是否为有效数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.1 排序\n",
    "\n",
    "- sort(a, dim=1, descending=False)\n",
    "    1. dim指定要排序的轴比如在dim=0表示在以某一列为单位进行排序\n",
    "    2. dim=1,以某一行为单位进行排序\n",
    "    3. descending : True : 降序排列, False 升序排列"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.return_types.sort(\n",
      "values=tensor([[1, 3, 4, 4, 5],\n",
      "        [1, 2, 3, 3, 5]]),\n",
      "indices=tensor([[0, 3, 1, 2, 4],\n",
      "        [2, 0, 1, 3, 4]]))\n",
      "torch.Size([2, 5])\n",
      "torch.return_types.sort(\n",
      "values=tensor([[1, 3, 1, 3, 5],\n",
      "        [2, 4, 4, 3, 5]]),\n",
      "indices=tensor([[0, 1, 1, 0, 0],\n",
      "        [1, 0, 0, 1, 1]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 4, 4, 3, 5],\n",
    "                  [2, 3, 1, 3, 5]])\n",
    "print(a.shape)\n",
    "print(torch.sort(a, dim=1,\n",
    "                 descending=False))\n",
    "\n",
    "\n",
    "b = torch.tensor([[1, 4, 4, 3, 5],\n",
    "                  [2, 3, 1, 3, 5]])\n",
    "print(b.shape)\n",
    "print(torch.sort(b, dim=0,\n",
    "                 descending=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2 TopK\n",
    "\n",
    "- torch.topk(a, k=2, dim=1, largest=False)\n",
    "    1. 获取前 k 的数据\n",
    "    2. dim=0, 表示以列为单位统计, 比如 第一列的前k大(小)的数据,dim=1表示以行为单位\n",
    "    3. largest : 如果为True, 表示获取最大的k个数据, 如果为False,表示获取最小的k个数据\n",
    "- torch.kthvalue(a, k=2, dim=0)\n",
    "    1. 获取第k大(小)的数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " torch.Size([2, 5])\n",
      "\n",
      " torch.return_types.topk(\n",
      "values=tensor([[1, 2],\n",
      "        [0, 1]]),\n",
      "indices=tensor([[3, 0],\n",
      "        [0, 3]]))\n",
      "\n",
      " torch.return_types.kthvalue(\n",
      "values=tensor([2, 4, 5, 1, 5]),\n",
      "indices=tensor([0, 0, 1, 1, 0]))\n",
      "\n",
      " torch.return_types.kthvalue(\n",
      "values=tensor([2, 1]),\n",
      "indices=tensor([0, 3]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[2, 4, 3, 1, 5],\n",
    "                  [0, 3, 5, 1, 4]])\n",
    "print(\"\\n\",a.shape)\n",
    "\n",
    "# 获取最小的前2个数据\n",
    "# a 的第一行最小的两个数据是 1,2\n",
    "# a 的第二行最小的两个数据是 0,1\n",
    "print(\"\\n\",torch.topk(a, k=2, dim=1, largest=False))\n",
    "\n",
    "# 获取以列为单位的第二小的数据\n",
    "# 比如 : 第一列第二小的是2, 第二列第二小的是4\n",
    "print(\"\\n\",torch.kthvalue(a, k=2, dim=0))\n",
    "print(\"\\n\",torch.kthvalue(a, k=2, dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.3 判断数据无穷或有效\n",
    "\n",
    "- torch.isfinite(a)\n",
    "    1. 判断是否是无界值, 如果是无界值为True\n",
    "- torch.isinf(a/0)\n",
    "    1. 判断是否为无穷值, 如果是返回True\n",
    "- torch.isnan(a)\n",
    "    1. 判断是否为 nan 值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9370, 0.2305, 0.1475],\n",
      "        [0.3855, 0.3496, 0.7522]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "\n",
      " tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([False, False,  True])\n",
      "tensor([[0.8951, 0.0250, 0.2720],\n",
      "        [0.4033, 0.2682, 0.7609]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.0250, 0.2720],\n",
      "        [0.2682, 0.4033]]),\n",
      "indices=tensor([[1, 2],\n",
      "        [1, 0]]))\n",
      "torch.return_types.topk(\n",
      "values=tensor([[0.8951, 0.2720],\n",
      "        [0.7609, 0.4033]]),\n",
      "indices=tensor([[0, 2],\n",
      "        [2, 0]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(a/0)\n",
    "# 非0值除以0会得到无限大的值\n",
    "# 判断是否为无界值(无穷大)\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isfinite(a/0))\n",
    "# 如果不是无界值会返回True\n",
    "print(torch.isinf(a/0))\n",
    "# 判断是否为Nan值\n",
    "print(\"\\n\", torch.isnan(a))\n",
    "\n",
    "import numpy as np\n",
    "a = torch.tensor([1, 2, np.nan])\n",
    "print(torch.isnan(a))\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(torch.topk(a, k=2, dim=1, largest=False))\n",
    "print(torch.topk(a, k=2, dim=1, largest=True))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 三角函数\n",
    "\n",
    "- torch.abs(input, out=None)\n",
    "- torch.acos(input, out=None)\n",
    "- torch.asin(input, out=None)\n",
    "- torch.atan(input, out=None)\n",
    "- torch.atan2(input, input2, out=None)\n",
    "- torch.cos(input, out=None)\n",
    "- torch.cosh(input, out=None)\n",
    "    1. 双曲余弦\n",
    "- torch.sin(input, out=None)\n",
    "- torch.sinh(input, out=None)\n",
    "- torch.tan(input, out=None)\n",
    "- torch.tanh(input, out=None)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3334, 0.8809, 0.6101],\n",
      "        [0.2979, 0.1444, 0.8445]])\n",
      "tensor([[0.9449, 0.6365, 0.8196],\n",
      "        [0.9560, 0.9896, 0.6641]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.cos(a)\n",
    "print(a)\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.7100e+25, 5.3432e+12])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([60,30])\n",
    "print(torch.cosh(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6.tensor中的其他函数\n",
    "\n",
    "- torch.abs()\n",
    "    1. 绝对值\n",
    "- torch.sign()\n",
    "    1. sgn函数, 也叫阶跃函数\n",
    "- [torch.erf](https://pytorch.org/docs/stable/special.html#torch.special.erf)\n",
    "- [torch.lerp](https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp)\n",
    "- torch.erfinv()\n",
    "- torch.addcdiv()\n",
    "- torch.sigmoid()\n",
    "- torch.addcmul()\n",
    "- torch.neg()\n",
    "- torch.cumprod()\n",
    "- torch.reciprocal()\n",
    "- torch.cumsum()\n",
    "- torch.rsqrt()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}