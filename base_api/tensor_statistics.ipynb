{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Pytorch的统计学函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for module in torch,:\n",
    "    print(module.__name__, module.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 均值与最大最小值\n",
    "\n",
    "- torch.mean() 返回平均值\n",
    "- torch.sum() 返回总和\n",
    "- torch.prod() 计算所有元素的积\n",
    "- torch.max() 返回最大值\n",
    "- torch.min() 返回最小值\n",
    "- torch.argmax() 返回最大值排序的索引值\n",
    "- torch.argmin() 返回最小值排序的索引值\n",
    "- 注意 :\n",
    "    1. dim=0, 表示列, dim=1 表示行\n",
    "    2. 返回每一列中最大值的索引\n",
    "    3. 比如 : 第一列是 4, 5 , 最大值5的索引是0\n",
    "    4. 第二列是 3, 6 , 最大值6的索引是1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [5., 6.]])\n",
      "tensor([4.5000, 4.5000])\n",
      "tensor([9., 9.])\n",
      "tensor([20., 18.])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[4, 3],\n",
    "                  [5, 6]], dtype=torch.float32)\n",
    "print(a)\n",
    "print(torch.mean(a, dim=0))\n",
    "print(torch.sum(a, dim=0))\n",
    "print(torch.prod(a, dim=0))\n",
    "\n",
    "# dim=0, 表示列, dim=1 表示行\n",
    "# 返回每一列中最大值的索引\n",
    "# 比如 : 第一列是 4, 5 , 最大值5的索引是0\n",
    "# 第二列是 3, 6 , 最大值6的索引是1\n",
    "print(torch.argmax(a, dim=0))\n",
    "print(torch.argmin(a, dim=0))\n",
    "\n",
    "# 以行为单位, 行内最大最小元素的索引\n",
    "print(torch.argmax(a, dim=1))\n",
    "print(torch.argmin(a, dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 方差与众数\n",
    "\n",
    "- torch.std() 返回标准差\n",
    "- torch.var() 返回方差\n",
    "- torch.median() 返回中间值\n",
    "- torch.mode() 返回众数值\n",
    "- torch.histc() 计算input的直方图\n",
    "    1. histc(a, bins = 6, min=0, max=0)\n",
    "    2. bins : 统计多少个区间\n",
    "    3. max/min : 定义最大值和最小值, 默认值取值为0,0 表示取tensor中的最大值和最小值\n",
    "- torch.bincount() 返回每个值出现的次数\n",
    "    1. 返回 0 ~ 最大值(tensor中的最大值) 出现的频次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2323)\n",
      "tensor(0.0539)\n",
      "tensor(0.4822)\n",
      "torch.return_types.mode(\n",
      "values=tensor([0.3132, 0.7638]),\n",
      "indices=tensor([1, 1]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "\n",
    "print(torch.std(a))\n",
    "print(torch.var(a))\n",
    "print(torch.median(a))\n",
    "print(torch.mode(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.6349, 0.9091],\n",
      "        [5.2806, 0.3580]])\n",
      "tensor([2., 0., 0., 0., 1., 1.])\n",
      "tensor([1, 1, 2, 2, 3, 6])\n",
      "tensor([0, 2, 2, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "#\n",
    "print(torch.histc(a, bins=6, min=0, max=0))\n",
    "\n",
    "a = torch.tensor([1, 1, 2, 2, 3, 6])\n",
    "print(a)\n",
    "# 返回 0 ~ 最大值 出现的频次\n",
    "print(torch.bincount(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Pytorch与分布函数\n",
    "\n",
    "- Tensor的torch.distributions\n",
    "    1. distributions 包含可参数化的概率分布和采样函数得分函数\n",
    "    2. 强化学习中策略梯度方法的基础pathwise derivative估计器\n",
    "    3. 变分自动编码器中的重新参数化技巧"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Pytorch与随机抽样\n",
    "\n",
    "- torch.manual_seed(seed) 定义随机种子\n",
    "- torch.normal() 定义随机数满足的分布\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7825, 0.7358]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义随机数种子\n",
    "torch.manual_seed(1)\n",
    "mean = torch.rand(1, 2)  # 均值\n",
    "std = torch.rand(1, 2)  # 方差\n",
    "print(torch.normal(mean, std))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Pytorch的范数运算\n",
    "\n",
    "- 在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即\n",
    "    1. 非负性;\n",
    "    2. 齐次性;\n",
    "    3. 三角不等式。\n",
    "- 常被用来度量某个向量空间(或矩阵)中的每个向量的长度或大小。\n",
    "- 0范数/1范数/2范数/p范数/核范数\n",
    "    1. torch.dist(input, other,p=2) 计算p范数\n",
    "    2. torch.norm() 计算2范数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5695],\n",
      "        [0.4388]]) tensor([[0.6387],\n",
      "        [0.5247]])\n",
      "tensor(0.1551)\n",
      "tensor(0.1103)\n",
      "tensor(0.0988)\n",
      "tensor(0.7189)\n",
      "tensor(0.6457)\n",
      "tensor(0.7189)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(2, 1)\n",
    "print(a, b)\n",
    "# 计算L1距离\n",
    "print(torch.dist(a, b, p=1))\n",
    "# L2\n",
    "print(torch.dist(a, b, p=2))\n",
    "# L3\n",
    "print(torch.dist(a, b, p=3))\n",
    "\n",
    "# a 的2范数\n",
    "print(torch.norm(a))\n",
    "print(torch.norm(a, p=3))\n",
    "print(torch.norm(a, p='fro'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Tensor的矩阵分解\n",
    "\n",
    "- LU分解:将矩阵A分解成L（下三角矩阵和U(上三角）矩阵的乘积\n",
    "- QR分解:将原矩阵分解成一个正交矩阵Q和一个上三角矩阵R的乘积\n",
    "- EVD分解:特征值分解\n",
    "- SVD分解:奇异值分解\n",
    "- 特征值分解\n",
    "    1. 将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法\n",
    "    2. 特征值VS特征向量\n",
    "- PCA与特征值分解\n",
    "    1. PCA:将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征\n",
    "    2. PCA算法的优化目标就是:\n",
    "        - 降维后同一纬度的方差最大\n",
    "        - 不同维度之间的相关性为0\n",
    "        - 协方差矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Tensor的张量裁剪\n",
    "\n",
    "- 对Tensor中的元素进行范围过滤\n",
    "- 常用于梯度裁剪(gradient clipping), 即在发生梯度离散或者梯度爆炸时对梯度的处理\n",
    "- a.clamp(min,max) 将张量裁剪到 (min, max) 的范围"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.8261, 3.0515],\n",
      "        [4.6355, 4.5499]])\n",
      "tensor([[4.0000, 3.0515],\n",
      "        [4.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2) * 10\n",
    "\n",
    "print(a)\n",
    "# 将\n",
    "a = a.clamp(2, 4)\n",
    "\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Tensor的索引与数据筛选\n",
    "\n",
    "- torch.where(condition, x, y)\n",
    "    1. 按照条件从x和y中选出满足条件的元素组成新tensor\n",
    "- torch.gather(input, dim, index, out=None)\n",
    "    1. 在指定维度上按照索引赋值输出tensor\n",
    "- torch.index_select(input, dim, index, out=None)\n",
    "    1. 按照指定索引输出tensor\n",
    "- torch.masked_select(input,mask, out=None)\n",
    "    1. 按照mask输出tensor, 输出为向量\n",
    "- torch.take(input, indices)\n",
    "    1. 将输入看成1D-tensor，按照索引得到输出tensor\n",
    "- torch.nonzero(input, out=None)\n",
    "    1. 输出非0元素的坐标"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.1 torch.where\n",
    "\n",
    "- torch.where(condition, x, y)\n",
    "    1. 按照条件从x和y中选出满足条件的元素组成新tensor\n",
    "    2. 例如 : condition = x > 5, 如果a有满足条件的数据, 就取a的数据, 否则取b对应位置的数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[7, 2, 3],\n",
      "        [4, 3, 6],\n",
      "        [7, 8, 2]])\n",
      "tensor([[7, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "(tensor([1, 1, 1, 2, 2, 2]), tensor([0, 1, 2, 0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "b = torch.tensor([[7, 2, 3],\n",
    "                  [4, 3, 6],\n",
    "                  [7, 8, 2]])\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.where(a > 3, a, b)\n",
    "\n",
    "print(out)\n",
    "\n",
    "c = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "print(torch.where(a > 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.2 torch.index_select\n",
    "\n",
    "- torch.index_select(input, dim, index, out=None)\n",
    "    1. 按照指定索引输出tensor\n",
    "    2. dim = 0, 指定以列为单位, 根据索引查询数据也是以列为单位\n",
    "    3. 比如 : dim=0 , index=[0, 1, 2] 表示第一列中, 索引为 [0,1,2]的数据, 第二列的[0,1,2]的数据 ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.3138, 0.1980, 0.4162, 0.2843],\n",
      "        [0.3398, 0.5239, 0.7981, 0.7718],\n",
      "        [0.0112, 0.8100, 0.6397, 0.9743]])\n",
      "tensor([[0.5725, 0.4980, 0.9371, 0.6556],\n",
      "        [0.3138, 0.1980, 0.4162, 0.2843],\n",
      "        [0.3398, 0.5239, 0.7981, 0.7718]]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "# dim = 0, 指定以列为单位, 根据索引查询数据也是以列为单位\n",
    "out = torch.index_select(a, dim=0,\n",
    "                         index=torch.tensor([0, 1, 2]))\n",
    "\n",
    "print(out, out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.3 torch.gather\n",
    "\n",
    "- torch.gather(input, dim, index, out=None)\n",
    "    1. 在指定维度上按照索引赋值输出tensor\n",
    "- tensor.view()\n",
    "    1. 可以修改tensor的shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([[ 1.,  6.,  3.,  8.],\n",
      "        [ 1.,  6., 11., 12.],\n",
      "        [ 1.,  6., 15., 16.]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 生成范围在1到16之间16个间隔相同的数据\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "\n",
    "print(a)\n",
    "\n",
    "# 索引的计算方式\n",
    "# dim = 0, 以列为单位\n",
    "# index = [[0, 1, 0, 1],\n",
    "#          [0, 1, 2, 2]]]\n",
    "# 如上, 第一个 [0, 1, 0, 1] 分别对应 第一列到第四列的索引为 [0, 1, 0, 1]的数据\n",
    "#\n",
    "out = torch.gather(a, dim=0,\n",
    "                   index=torch.tensor([[0, 1, 0, 1],\n",
    "                                       [0, 1, 2, 2],\n",
    "                                       [0, 1, 3, 3]]))\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[ 5., 10.],\n",
      "        [ 5., 10.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "print(b)\n",
    "out = torch.gather(a, dim=0,\n",
    "                   index=torch.tensor([\n",
    "                       [1, 2],\n",
    "                       [1, 2]\n",
    "                   ]))\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.4 torch.masked_index\n",
    "\n",
    "- torch.masked_select(input,mask, out=None)\n",
    "    1. 按照mask输出tensor, 输出为向量\n",
    "    2. mask 相当于是一个条件, 返回结果是一个布尔类型的矩阵, 满足mask条件的返回True,\n",
    "        否则返回False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [ True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True]])\n",
      "tensor([ 9., 10., 11., 12., 13., 14., 15., 16.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "# 设置条件\n",
    "mask = torch.gt(a, 8)\n",
    "print(a)\n",
    "print(mask)\n",
    "out = torch.masked_select(a, mask)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.5 torch.take\n",
    "\n",
    "- torch.take(input, indices)\n",
    "    1. 将输入看成1D-tensor，按照索引得到输出tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1., 16., 14., 11.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "b = torch.take(a, index=torch.tensor([0, 15, 13, 10]))\n",
    "print(b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.6 torch.nonzero\n",
    "\n",
    "- torch.nonzero(input, out=None)\n",
    "    1. 输出非0元素的坐标"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 3]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[0, 1, 2, 0], [2, 3, 0, 1]])\n",
    "out = torch.nonzero(a)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8. 张量的组合与拼接\n",
    "\n",
    "- torch.cat(seq, dim=O, out=None)\n",
    "    1. 按照已经存在的维度进行拼接\n",
    "- torch.stack(seq, dim=0, out=None)\n",
    "    1. 按照新的维度进行拼接\n",
    "- torch.gather(input, dim, index, out=None)\n",
    "    1. 在指定维度上按照索赋值输出tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.1 torch.cat\n",
    "\n",
    "- torch.cat(seq, dim=O, out=None)\n",
    "    1. 按照已经存在的维度进行拼接"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros((2, 4))\n",
    "b = torch.ones((2, 4))\n",
    "\n",
    "# 上下拼接\n",
    "out = torch.cat((a, b), dim=0)\n",
    "print(out)\n",
    "\n",
    "# 左右拼接\n",
    "out = torch.cat((a, b), dim=1)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 8.1 torch.stack\n",
    "\n",
    "- torch.stack(seq, dim=0, out=None)\n",
    "    1. 按照新的维度进行拼接"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.]],\n",
      "\n",
      "        [[ 7.,  8.,  9.],\n",
      "         [10., 11., 12.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.],\n",
      "         [10., 11., 12.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 1.,  7.],\n",
      "         [ 2.,  8.],\n",
      "         [ 3.,  9.]],\n",
      "\n",
      "        [[ 4., 10.],\n",
      "         [ 5., 11.],\n",
      "         [ 6., 12.]]])\n",
      "torch.Size([2, 3, 2])\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.linspace(1, 6, 6).view(2, 3)\n",
    "b = torch.linspace(7, 12, 6).view(2, 3)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.stack((a, b), dim=0)\n",
    "print(out)\n",
    "# torch.Size([2, 2, 3])\n",
    "print(out.shape)\n",
    "\n",
    "out = torch.stack((a, b), dim=1)\n",
    "print(out)\n",
    "# torch.Size([2, 2, 3])\n",
    "print(out.shape)\n",
    "\n",
    "out = torch.stack((a, b), dim=2)\n",
    "print(out)\n",
    "# torch.Size([2, 3, 2])\n",
    "print(out.shape)\n",
    "\n",
    "print(out[:, :, 0])\n",
    "print(out[:, :, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 9. 张量的切片\n",
    "\n",
    "- torch.chunk(tensor, chunks, dim=0)\n",
    "    1. 按照某个维度平均分块(最后一个可能小于平均值)\n",
    "- torch.split(tensor, split_size_or_sections, dim=0)\n",
    "    1. 按照某个维度依照第二个参数给出的list或者int进行分割tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.1 torch.chunk\n",
    "- torch.chunk(tensor, chunks, dim=0)\n",
    "    1. 按照某个维度平均分块(最后一个可能小于平均值)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1484, 0.1227, 0.5304, 0.4148],\n",
      "        [0.7937, 0.2104, 0.0555, 0.8639],\n",
      "        [0.4259, 0.7812, 0.6607, 0.1251]])\n",
      "tensor([[0.1484, 0.1227, 0.5304, 0.4148],\n",
      "        [0.7937, 0.2104, 0.0555, 0.8639]]) torch.Size([2, 4])\n",
      "tensor([[0.4259, 0.7812, 0.6607, 0.1251]]) torch.Size([1, 4])\n",
      "tensor([[0.1484, 0.1227],\n",
      "        [0.7937, 0.2104],\n",
      "        [0.4259, 0.7812]]) torch.Size([3, 2])\n",
      "tensor([[0.5304, 0.4148],\n",
      "        [0.0555, 0.8639],\n",
      "        [0.6607, 0.1251]]) torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand((3, 4))\n",
    "print(a)\n",
    "# 返回类型是tuple\n",
    "\n",
    "# 以第一个维度为划分, 分成两个 tensor\n",
    "out = torch.chunk(a, chunks=2, dim=0)\n",
    "print(out[0], out[0].shape)\n",
    "print(out[1], out[1].shape)\n",
    "\n",
    "out = torch.chunk(a, 2, dim=1)\n",
    "print(out[0], out[0].shape)\n",
    "print(out[1], out[1].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 9.2 torch.split\n",
    "\n",
    "- torch.split(tensor, split_size_or_sections, dim=0)\n",
    "    1. 按照某个维度依照第二个参数给出的list或者int进行分割tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6004, 0.6201, 0.1652, 0.2628],\n",
      "        [0.6705, 0.5896, 0.2873, 0.3486],\n",
      "        [0.9579, 0.4075, 0.7819, 0.7165],\n",
      "        [0.1768, 0.0748, 0.9799, 0.5261],\n",
      "        [0.8427, 0.6036, 0.6608, 0.8735],\n",
      "        [0.9741, 0.1682, 0.5625, 0.8731],\n",
      "        [0.8622, 0.8106, 0.1381, 0.1399],\n",
      "        [0.1976, 0.5628, 0.9983, 0.1842],\n",
      "        [0.7664, 0.2233, 0.0299, 0.3937],\n",
      "        [0.7881, 0.9642, 0.1895, 0.6085]])\n",
      "4\n",
      "tensor([[0.6004, 0.6201, 0.1652, 0.2628],\n",
      "        [0.6705, 0.5896, 0.2873, 0.3486],\n",
      "        [0.9579, 0.4075, 0.7819, 0.7165]]) torch.Size([3, 4])\n",
      "tensor([[0.1768, 0.0748, 0.9799, 0.5261],\n",
      "        [0.8427, 0.6036, 0.6608, 0.8735],\n",
      "        [0.9741, 0.1682, 0.5625, 0.8731]]) torch.Size([3, 4])\n",
      "tensor([[0.8622, 0.8106, 0.1381, 0.1399],\n",
      "        [0.1976, 0.5628, 0.9983, 0.1842],\n",
      "        [0.7664, 0.2233, 0.0299, 0.3937]]) torch.Size([3, 4])\n",
      "tensor([[0.7881, 0.9642, 0.1895, 0.6085]]) torch.Size([1, 4])\n",
      "tensor([[0.6004, 0.6201, 0.1652, 0.2628]]) torch.Size([1, 4])\n",
      "tensor([[0.6705, 0.5896, 0.2873, 0.3486],\n",
      "        [0.9579, 0.4075, 0.7819, 0.7165],\n",
      "        [0.1768, 0.0748, 0.9799, 0.5261]]) torch.Size([3, 4])\n",
      "tensor([[0.8427, 0.6036, 0.6608, 0.8735],\n",
      "        [0.9741, 0.1682, 0.5625, 0.8731],\n",
      "        [0.8622, 0.8106, 0.1381, 0.1399],\n",
      "        [0.1976, 0.5628, 0.9983, 0.1842],\n",
      "        [0.7664, 0.2233, 0.0299, 0.3937],\n",
      "        [0.7881, 0.9642, 0.1895, 0.6085]]) torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((10, 4))\n",
    "print(a)\n",
    "# 分隔成3个tensor\n",
    "out = torch.split(a, 3, dim=0)\n",
    "print(len(out))\n",
    "# 分隔成3个[3, 4], 剩下一个为[1, 4]\n",
    "for t in out:\n",
    "    print(t, t.shape)\n",
    "\n",
    "# 在第一维度上分割成, 1, 3, 6 三个大小的tensor\n",
    "# 分别是 : [1,4] [3,4] [6, 4]\n",
    "out = torch.split(a, [1, 3, 6], dim=0)\n",
    "for t in out:\n",
    "    print(t, t.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  10. Tensor的变形操作\n",
    "\n",
    "- torch.reshape(input, shape)\n",
    "- torch.t(input)\n",
    "    1. 只针对2D tensor转置\n",
    "- torch.transpose(input, dim0, dim1)\n",
    "    1. 交换两个维度\n",
    "- torch.squeeze(input, dim= None, out= None)\n",
    "    1. 去除那些维度大小为1的维度\n",
    "- torch.unbind(tensor, dim=0)\n",
    "    1. 去除某个维度\n",
    "- torch.unsqueeze(input, dim, out=None)\n",
    "    1. 在指定位置添加维度\n",
    "- torch.flip(input, dims)\n",
    "    1. 按照给定维度翻转张量\n",
    "- torch.rot90(input, k, dims)\n",
    "    1. 按照指定维度和旋转次数进行张量旋转"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.1 torch.reshape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1581, 0.0801, 0.2709],\n",
      "        [0.4418, 0.1935, 0.6829]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "out = torch.reshape(a, (3, 2))\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.2 torch.transpose\n",
    "\n",
    "- torch.t(out) : 针对二维向量转置\n",
    "- torch.transpose(out, dim0 = 0, dim1 = 1)\n",
    "    1. 作用是指定维度进行转置\n",
    "    2. dim0, dim1 是需要交换的维度, 0 是第一个维度, 1 是第二个维度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3307, 0.9885, 0.4422],\n",
      "        [0.4828, 0.0281, 0.1782]])\n",
      "tensor([[0.3307, 0.9885],\n",
      "        [0.4422, 0.4828],\n",
      "        [0.0281, 0.1782]])\n",
      "tensor([[0.3307, 0.4422, 0.0281],\n",
      "        [0.9885, 0.4828, 0.1782]])\n",
      "tensor([[0.3307, 0.4422, 0.0281],\n",
      "        [0.9885, 0.4828, 0.1782]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "\n",
    "out = torch.reshape(a, (3, 2))\n",
    "print(out)\n",
    "print(torch.t(out))\n",
    "# 指定要交换的维度 , dim0 = 0 , dim=1\n",
    "print(torch.transpose(out, 0, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.3 torch.squeeze\n",
    "\n",
    "- torch.squeeze(input, dim= None, out= None)\n",
    "    1. 去除那些维度大小为1的维度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6601, 0.8238, 0.2413],\n",
      "         [0.6084, 0.3180, 0.3877]]])\n",
      "tensor([[0.6601, 0.8238, 0.2413],\n",
      "        [0.6084, 0.3180, 0.3877]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# 根据括号[] 判断维度\n",
    "a = torch.rand(1, 2, 3)\n",
    "print(a)\n",
    "out = torch.squeeze(a)\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.4 torch.unsqueeze\n",
    "- torch.unsqueeze(input, dim, out=None)\n",
    "    1. 在指定位置添加维度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "out = torch.unsqueeze(a, dim=0)\n",
    "print(out.shape)\n",
    "\n",
    "out = torch.unsqueeze(a, dim=1)\n",
    "print(out.shape)\n",
    "\n",
    "out = torch.unsqueeze(a, dim=-1)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.5 torch.unbind\n",
    "\n",
    "- torch.unbind(tensor, dim=0)\n",
    "    1. 去除某个维度\n",
    "    2. 去除某个维度后会得到一个 tuple"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9891, 0.1500, 0.6211],\n",
      "         [0.1303, 0.9269, 0.3060]]])\n",
      "(tensor([[0.9891, 0.1500, 0.6211],\n",
      "        [0.1303, 0.9269, 0.3060]]),)\n",
      "(tensor([[0.9891, 0.1500, 0.6211]]), tensor([[0.1303, 0.9269, 0.3060]]))\n",
      "(tensor([[0.9891, 0.1303]]), tensor([[0.1500, 0.9269]]), tensor([[0.6211, 0.3060]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 2, 3)\n",
    "\n",
    "print(a)\n",
    "out = torch.unbind(a, dim=0)\n",
    "print(out)\n",
    "\n",
    "out = torch.unbind(a, dim=1)\n",
    "print(out)\n",
    "\n",
    "out = torch.unbind(a, dim=2)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.6 torch.flip\n",
    "\n",
    "- torch.flip(input, dims)\n",
    "    1. 按照给定维度翻转张量\n",
    "    2. dims=[1,2] 对1和2维度上的数据进行翻转"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5802, 0.6525, 0.0502],\n",
      "         [0.8643, 0.9359, 0.9133]]])\n",
      "tensor([[[0.9133, 0.9359, 0.8643],\n",
      "         [0.0502, 0.6525, 0.5802]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 2, 3)\n",
    "print(a)\n",
    "# 对 1, 2维的数据进行翻转\n",
    "print(torch.flip(a, dims=[1, 2]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 10.7 torch.rot90\n",
    "\n",
    "- torch.rot90(input, k, dims)\n",
    "    1. 按照指定维度和旋转次数进行张量旋转"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5802, 0.6525, 0.0502],\n",
      "         [0.8643, 0.9359, 0.9133]]])\n",
      "tensor([[[0.5802],\n",
      "         [0.8643]],\n",
      "\n",
      "        [[0.6525],\n",
      "         [0.9359]],\n",
      "\n",
      "        [[0.0502],\n",
      "         [0.9133]]])\n",
      "torch.Size([3, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "out = torch.rot90(a, -1, dims=[0, 2])\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 11. 张量填充\n",
    "\n",
    "- 定义tensor并填充指定的值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 11.1 torch.full\n",
    "\n",
    "- torch.full(size=(2, 3), fill_value=3.14)\n",
    "    1. 填充指定的值"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full(size=(2, 3), fill_value=3.14)\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 12. Tensor的频谱操作\n",
    "\n",
    "- torch.fft(input, signal ndim, normalized= False)\n",
    "- torch.ifft(input, signal ndim, normalized=False)\n",
    "- torch.rfft(input, signal_ ndim, normalized= False, onesided= True)\n",
    "- torch.irfft(input, signal_ ndim, normalized= False, onesided=True)\n",
    "- torch.stft(signa, frame_ length, hop, ..)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}