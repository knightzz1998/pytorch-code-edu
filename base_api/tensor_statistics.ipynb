{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Pytorch的统计学函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch 1.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for module in torch,:\n",
    "    print(module.__name__, module.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 均值与最大最小值\n",
    "\n",
    "- torch.mean() 返回平均值\n",
    "- torch.sum() 返回总和\n",
    "- torch.prod() 计算所有元素的积\n",
    "- torch.max() 返回最大值\n",
    "- torch.min() 返回最小值\n",
    "- torch.argmax() 返回最大值排序的索引值\n",
    "- torch.argmin() 返回最小值排序的索引值\n",
    "- 注意 :\n",
    "    1. dim=0, 表示列, dim=1 表示行\n",
    "    2. 返回每一列中最大值的索引\n",
    "    3. 比如 : 第一列是 4, 5 , 最大值5的索引是0\n",
    "    4. 第二列是 3, 6 , 最大值6的索引是1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 3.],\n",
      "        [5., 6.]])\n",
      "tensor([4.5000, 4.5000])\n",
      "tensor([9., 9.])\n",
      "tensor([20., 18.])\n",
      "tensor([1, 1])\n",
      "tensor([0, 0])\n",
      "tensor([0, 1])\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[4, 3],\n",
    "                  [5, 6]], dtype=torch.float32)\n",
    "print(a)\n",
    "print(torch.mean(a, dim=0))\n",
    "print(torch.sum(a, dim=0))\n",
    "print(torch.prod(a, dim=0))\n",
    "\n",
    "# dim=0, 表示列, dim=1 表示行\n",
    "# 返回每一列中最大值的索引\n",
    "# 比如 : 第一列是 4, 5 , 最大值5的索引是0\n",
    "# 第二列是 3, 6 , 最大值6的索引是1\n",
    "print(torch.argmax(a, dim=0))\n",
    "print(torch.argmin(a, dim=0))\n",
    "\n",
    "# 以行为单位, 行内最大最小元素的索引\n",
    "print(torch.argmax(a, dim=1))\n",
    "print(torch.argmin(a, dim=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 方差与众数\n",
    "\n",
    "- torch.std() 返回标准差\n",
    "- torch.var() 返回方差\n",
    "- torch.median() 返回中间值\n",
    "- torch.mode() 返回众数值\n",
    "- torch.histc() 计算input的直方图\n",
    "    1. histc(a, bins = 6, min=0, max=0)\n",
    "    2. bins : 统计多少个区间\n",
    "    3. max/min : 定义最大值和最小值, 默认值取值为0,0 表示取tensor中的最大值和最小值\n",
    "- torch.bincount() 返回每个值出现的次数\n",
    "    1. 返回 0 ~ 最大值(tensor中的最大值) 出现的频次"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2214)\n",
      "tensor(0.0490)\n",
      "tensor(0.3930)\n",
      "torch.return_types.mode(\n",
      "values=tensor([0.3930, 0.0326]),\n",
      "indices=tensor([0, 0]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2)\n",
    "\n",
    "print(torch.std(a))\n",
    "print(torch.var(a))\n",
    "print(torch.median(a))\n",
    "print(torch.mode(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8473, 0.1245],\n",
      "        [6.4012, 2.8132]])\n",
      "tensor([2., 0., 1., 0., 0., 1.])\n",
      "tensor([1, 1, 2, 2, 3, 6])\n",
      "tensor([0, 2, 2, 1, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "#\n",
    "print(torch.histc(a, bins=6, min=0, max=0))\n",
    "\n",
    "a = torch.tensor([1, 1, 2, 2, 3, 6])\n",
    "print(a)\n",
    "# 返回 0 ~ 最大值 出现的频次\n",
    "print(torch.bincount(a))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Pytorch与分布函数\n",
    "\n",
    "- Tensor的torch.distributions\n",
    "    1. distributions 包含可参数化的概率分布和采样函数得分函数\n",
    "    2. 强化学习中策略梯度方法的基础pathwise derivative估计器\n",
    "    3. 变分自动编码器中的重新参数化技巧"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Pytorch与随机抽样\n",
    "\n",
    "- torch.manual_seed(seed) 定义随机种子\n",
    "- torch.normal() 定义随机数满足的分布\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7825, 0.7358]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 定义随机数种子\n",
    "torch.manual_seed(1)\n",
    "mean = torch.rand(1, 2)  # 均值\n",
    "std = torch.rand(1, 2)  # 方差\n",
    "print(torch.normal(mean, std))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Pytorch的范数运算\n",
    "\n",
    "- 在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即\n",
    "    1. 非负性;\n",
    "    2. 齐次性;\n",
    "    3. 三角不等式。\n",
    "- 常被用来度量某个向量空间(或矩阵)中的每个向量的长度或大小。\n",
    "- 0范数/1范数/2范数/p范数/核范数\n",
    "    1. torch.dist(input, other,p=2) 计算p范数\n",
    "    2. torch.norm() 计算2范数"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6826],\n",
      "        [0.3051]]) tensor([[0.4635],\n",
      "        [0.4550]])\n",
      "tensor(0.3689)\n",
      "tensor(0.2654)\n",
      "tensor(0.2403)\n",
      "tensor(0.7477)\n",
      "tensor(0.7024)\n",
      "tensor(0.7477)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 1)\n",
    "b = torch.rand(2, 1)\n",
    "print(a, b)\n",
    "# 计算L1距离\n",
    "print(torch.dist(a, b, p=1))\n",
    "# L2\n",
    "print(torch.dist(a, b, p=2))\n",
    "# L3\n",
    "print(torch.dist(a, b, p=3))\n",
    "\n",
    "# a 的2范数\n",
    "print(torch.norm(a))\n",
    "print(torch.norm(a, p=3))\n",
    "print(torch.norm(a, p='fro'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Tensor的矩阵分解\n",
    "\n",
    "- LU分解:将矩阵A分解成L（下三角矩阵和U(上三角）矩阵的乘积\n",
    "- QR分解:将原矩阵分解成一个正交矩阵Q和一个上三角矩阵R的乘积\n",
    "- EVD分解:特征值分解\n",
    "- SVD分解:奇异值分解\n",
    "- 特征值分解\n",
    "    1. 将矩阵分解为由其特征值和特征向量表示的矩阵之积的方法\n",
    "    2. 特征值VS特征向量\n",
    "- PCA与特征值分解\n",
    "    1. PCA:将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征\n",
    "    2. PCA算法的优化目标就是:\n",
    "        - 降维后同一纬度的方差最大\n",
    "        - 不同维度之间的相关性为0\n",
    "        - 协方差矩阵"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Tensor的张量裁剪\n",
    "\n",
    "- 对Tensor中的元素进行范围过滤\n",
    "- 常用于梯度裁剪(gradient clipping), 即在发生梯度离散或者梯度爆炸时对梯度的处理\n",
    "- a.clamp(min,max) 将张量裁剪到 (min, max) 的范围"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.3978, 5.2394],\n",
      "        [7.9806, 7.7177]])\n",
      "tensor([[3.3978, 4.0000],\n",
      "        [4.0000, 4.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.rand(2, 2) * 10\n",
    "\n",
    "print(a)\n",
    "# 将\n",
    "a = a.clamp(2, 4)\n",
    "\n",
    "print(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 7. Tensor的索引与数据筛选\n",
    "\n",
    "- torch.where(condition, x, y)\n",
    "    1. 按照条件从x和y中选出满足条件的元素组成新tensor\n",
    "- torch.gather(input, dim, index, out=None)\n",
    "    1. 在指定维度上按照索引赋值输出tensor\n",
    "- torch.index_select(input, dim, index, out=None)\n",
    "    1. 按照指定索引输出tensor\n",
    "- torch.masked_select(input,mask, out=None)\n",
    "    1. 按照mask输出tensor, 输出为向量\n",
    "- torch.take(input, indices)\n",
    "    1. 将输入看成1D-tensor，按照索引得到输出tensor\n",
    "- torch.nonzero(input, out=None)\n",
    "    1. 输出非0元素的坐标"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.1 torch.where\n",
    "\n",
    "- torch.where(condition, x, y)\n",
    "    1. 按照条件从x和y中选出满足条件的元素组成新tensor\n",
    "    2. 例如 : condition = x > 5, 如果a有满足条件的数据, 就取a的数据, 否则取b对应位置的数据"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[7, 2, 3],\n",
      "        [4, 3, 6],\n",
      "        [7, 8, 2]])\n",
      "tensor([[7, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "(tensor([1, 1, 1, 2, 2, 2]), tensor([0, 1, 2, 0, 1, 2]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "b = torch.tensor([[7, 2, 3],\n",
    "                  [4, 3, 6],\n",
    "                  [7, 8, 2]])\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "out = torch.where(a > 3, a, b)\n",
    "\n",
    "print(out)\n",
    "\n",
    "c = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "print(torch.where(a > 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.2 torch.index_select\n",
    "\n",
    "- torch.index_select(input, dim, index, out=None)\n",
    "    1. 按照指定索引输出tensor\n",
    "    2. dim = 0, 指定以列为单位, 根据索引查询数据也是以列为单位\n",
    "    3. 比如 : dim=0 , index=[0, 1, 2] 表示第一列中, 索引为 [0,1,2]的数据, 第二列的[0,1,2]的数据 ..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6818, 0.7479, 0.0369, 0.7517],\n",
      "        [0.1484, 0.1227, 0.5304, 0.4148],\n",
      "        [0.7937, 0.2104, 0.0555, 0.8639],\n",
      "        [0.4259, 0.7812, 0.6607, 0.1251]])\n",
      "tensor([[0.6818, 0.7479, 0.0369, 0.7517],\n",
      "        [0.1484, 0.1227, 0.5304, 0.4148],\n",
      "        [0.7937, 0.2104, 0.0555, 0.8639]]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "# dim = 0, 指定以列为单位, 根据索引查询数据也是以列为单位\n",
    "out = torch.index_select(a, dim=0,\n",
    "                         index=torch.tensor([0, 1, 2]))\n",
    "\n",
    "print(out, out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 7.4 torch.gather\n",
    "\n",
    "- torch.gather(input, dim, index, out=None)\n",
    "    1. 在指定维度上按照索引赋值输出tensor\n",
    "- tensor.view()\n",
    "    1. 可以修改tensor的shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16.]])\n",
      "tensor([[ 1.,  6.,  3.,  8.],\n",
      "        [ 1.,  6., 11., 12.],\n",
      "        [ 1.,  6., 15., 16.]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 生成范围在1到16之间16个间隔相同的数据\n",
    "a = torch.linspace(1, 16, 16).view(4, 4)\n",
    "\n",
    "print(a)\n",
    "\n",
    "# 索引的计算方式\n",
    "# dim = 0, 以列为单位\n",
    "# index = [[0, 1, 0, 1],\n",
    "#          [0, 1, 2, 2]]]\n",
    "# 如上, 第一个 [0, 1, 0, 1] 分别对应 第一列到第四列的索引为 [0, 1, 0, 1]的数据\n",
    "#\n",
    "out = torch.gather(a, dim=0,\n",
    "                   index=torch.tensor([[0, 1, 0, 1],\n",
    "                                       [0, 1, 2, 2],\n",
    "                                       [0, 1, 3, 3]]))\n",
    "print(out)\n",
    "print(out.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[ 5., 10.],\n",
      "        [ 5., 10.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "print(b)\n",
    "out = torch.gather(a, dim=0,\n",
    "                   index=torch.tensor([\n",
    "                       [1, 2],\n",
    "                       [1, 2]\n",
    "                   ]))\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}