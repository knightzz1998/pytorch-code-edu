{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 文本情感分类"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 读取数据训练词向量模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 读取微博评论数据"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119988 119988\n"
     ]
    }
   ],
   "source": [
    "weibo_senti_100k = pd.read_csv(\"../data/weibo_senti_100k.csv\", encoding=\"utf-8\")\n",
    "sentences = weibo_senti_100k['review']\n",
    "labels = weibo_senti_100k['label']\n",
    "\n",
    "print(len(sentences), len(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 读取停用词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "753\n"
     ]
    }
   ],
   "source": [
    "cn_stop_words = open(\"../data/cn_stopwords.txt\", mode=\"r\", encoding=\"utf-8\").readlines()\n",
    "cn_stop_words = [word.strip() for word in cn_stop_words]\n",
    "print(len(cn_stop_words))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分词并使用word2vec训练词向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\WANGTI~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.604 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202388\n",
      "119988\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = {} # 统计词频\n",
    "sentences_list = [] # 存储分词后的词组\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.strip()\n",
    "    seq_list = jieba.cut(sentence, cut_all=False)\n",
    "    seq_res = []\n",
    "    # 过滤停用词\n",
    "    for seq in seq_list:\n",
    "        if seq not in cn_stop_words:\n",
    "            seq_res.append(seq)\n",
    "        if seq in word_count_dict.keys():\n",
    "            word_count_dict[seq] += 1\n",
    "        else:\n",
    "            word_count_dict[seq] = 1\n",
    "    sentences_list.append(seq_res)\n",
    "\n",
    "print(len(word_count_dict))\n",
    "print(len(sentences_list))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 定义PAD 和 UNK\n",
    "UNK = \"<UNK>\" # 如果当前词词表中不存在时, 使用UNK\n",
    "PAD = \"<PAD>\" # 如果当前句子不满足指定长度, 使用PAD填充\n",
    "\n",
    "# 存储词表\n",
    "word_list = [word for word,count in word_count_dict.items()]\n",
    "word_idx_dict = {word:index for index,word in enumerate(word_list)}\n",
    "word_idx_dict.update({UNK:len(word_idx_dict), PAD: len(word_idx_dict) + 1})\n",
    "ff = open(\"out/weibo_dict\", \"w\", encoding=\"utf-8\")\n",
    "for item in word_idx_dict.keys():\n",
    "    ff.writelines(\"{},{}\\n\".format(item, word_idx_dict[item]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 使用word2vec训练词向量"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from model_config import RNNConfig, Word2VecConfig\n",
    "\n",
    "rnn_config = RNNConfig()\n",
    "word2vec_config = Word2VecConfig()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(11528071, 13765395)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=sentences_list,\n",
    "    min_count=1,\n",
    "    vector_size=word2vec_config.vector_size,\n",
    "    workers=word2vec_config.workers,\n",
    "    window=word2vec_config.window\n",
    ")\n",
    "\n",
    "word2vec_model.train(sentences_list,\n",
    "                     total_examples=word2vec_model.corpus_total_words, epochs=word2vec_model.epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "word2vec_model.save(\"out/weibo_word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DevApp\\anaconda3\\envs\\torch-env-gpu\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([100])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_tensor = torch.from_numpy(word2vec_model.wv['帅'])\n",
    "vec_tensor.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 将数据转换为词向量表"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 32\n",
    "sentences_len = len(sentences)\n",
    "PAD = np.ndarray(shape=(100,))\n",
    "UNK = np.ndarray(shape=(100,))\n",
    "print(PAD.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119988/119988 [01:05<00:00, 1832.56it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32mC:\\Users\\WANGTI~1\\AppData\\Local\\Temp/ipykernel_1208/1327845037.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     26\u001B[0m     \u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvector_seq\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 27\u001B[1;33m \u001B[0mdatasets\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     28\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdatasets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."
     ]
    }
   ],
   "source": [
    "\n",
    "data = []\n",
    "for i in tqdm(range(sentences_len)):\n",
    "\n",
    "    sentence = sentences[i]\n",
    "    label = labels[i]\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    label = int(label)\n",
    "    word_list = jieba.cut(sentence, cut_all=False)\n",
    "\n",
    "    vector_seq = []\n",
    "    # 过滤停用词\n",
    "    for seq in word_list:\n",
    "\n",
    "        if seq in cn_stop_words:\n",
    "            continue\n",
    "\n",
    "        if seq in word2vec_model.wv:\n",
    "            vector_seq.append(word2vec_model.wv[seq])\n",
    "        else:\n",
    "            vector_seq.append(UNK)\n",
    "    # 如果句子长度不够的话, 使用PAD进行填充\n",
    "    if len(vector_seq) < max_seq_len:\n",
    "        vector_seq += [PAD for i in range(max_seq_len - len(vector_seq))]\n",
    "\n",
    "    data.append([label, vector_seq])\n",
    "datasets = torch.from_numpy(np.array(data))\n",
    "print(datasets.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_size = len(word_list)\n",
    "# 初始化词向量表\n",
    "word_vectors = torch.randn([vocab_size, max_seq_len, 300])\n",
    "for i in range(0, vocab_size):\n",
    "    word = word_list[i]\n",
    "    if word in word2vec_model.wv:\n",
    "        vector = word2vec_model.wv[word]\n",
    "        word_vectors[i, :] = torch.from_numpy(vector)\n",
    "print(len(word_vectors))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word_vectors.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义模型"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WordEmbed(nn.Module):\n",
    "    def __init__(self, hidden_dim, seq_max_len, word_vectors, drop_prob=0.0):\n",
    "        \"\"\"\n",
    "            初始化, 并定义模型\n",
    "        :param embedding_dim: embedding后每个单词生成词向量的维度\n",
    "        :param seq_max_len: 句子最大长度\n",
    "        \"\"\"\n",
    "        super(WordEmbed).__init__()\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(word_vectors)  # 加载词向量\n",
    "        self.word_embeddings.weight.requires_grad = False  # 关闭计算\n",
    "        self.rnn_node = nn.RNN(word_vectors.size(1), hidden_dim, bidirectional=True, dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=1)\n",
    "\n",
    "    def forward(self, sentence_inputs, batch_size=128):\n",
    "        # shape size of :\n",
    "        #\n",
    "        embeds = self.word_embeddings(sentence_inputs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = WordEmbed()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}